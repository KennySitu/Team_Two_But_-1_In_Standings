{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "#import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Cell Above, I have declared all the necessary python libraries needed to scrape and clean the Data. It includes functionality such as selecting only the first page of the PDF since that is the only data we require, the libraries needed to scrape the data and select the desired columns using Beautiful Soup, and the abilitiy to write to a csv file so we can use it to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PDF_Page_Selector(aFileName):\n",
    "    '''\n",
    "    This Function takes one input: an Input File in pdf format, it then only selects the first page and outputs a\n",
    "    new pdf only containing that first page\n",
    "    '''\n",
    "    assert aFileName[-4:] == \".pdf\"\n",
    "    thePDFReader = PyPDF2.PdfFileReader(open(aFileName, \"rb\"))\n",
    "    theFirstPage = thePDFReader.getPage(0)  # extracting the first page since it contains the table we desire\n",
    "    theWriter = PyPDF2.PdfFileWriter()\n",
    "    theWriter.addPage(theFirstPage)\n",
    "    with open(\"TheArkansasTable.pdf\", \"wb\") as theOutputStream: #writing the first page to new PDF file\n",
    "        theWriter.write(theOutputStream)\n",
    "\n",
    "#PDF_Page_Selector('NSDUHsaeArkansas2016.pdf') #input file name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Module above, I ran it in Pycharm because I had troubles importing PyPDF2 into jupyterhub. The function above takes one input arguement, the file name of the file we would like to snip a page from, and creates a new file that only contains the page we desire. ex) [The Input File](NSDUHsaeCalifornia2016.pdf) and [The Output File](TheCaliforniaTable.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure\n",
      "Serious Mental Illness\n",
      "Any Mental Illness\n",
      "Received Mental Health Services \n",
      "Had Serious Thoughts of Suicide\n",
      "Major Depressive Episode \n",
      "\n",
      "18+\n",
      "93 \n",
      "404 \n",
      "256 \n",
      "95 \n",
      "162 \n",
      "\n",
      "['Measure', '18+']\n",
      "['Serious Mental Illness', '93 ']\n",
      "['Any Mental Illness', '404 ']\n",
      "['Received Mental Health Services ', '256 ']\n",
      "['Had Serious Thoughts of Suicide', '95 ']\n",
      "['Major Depressive Episode ', '162 ']\n"
     ]
    }
   ],
   "source": [
    "def Scraping_HTML_File(aFileName):\n",
    "    '''\n",
    "    This function takes an input html file, scrapes the data we need and cleans the numbers, commas, and unnecessary \n",
    "    data out\n",
    "    '''\n",
    "    assert aFileName[-5:] == \".html\"\n",
    "    theRowData = []\n",
    "    thePage = urllib.urlopen(aFileName)\n",
    "    theSoup = BeautifulSoup(thePage, 'html.parser')\n",
    "    #theDataHeader is the header and theData is the actual data\n",
    "    theDataHeader = theSoup.find('div', attrs = {'style' : 'position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:538px; top:143px; width:20px; height:150px;'})\n",
    "    theHeaderText = theDataHeader.text\n",
    "    theData = theSoup.find('div', attrs = {'style' : 'position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:538px; top:366px; width:20px; height:142px;'})\n",
    "    theDataText = theData.text\n",
    "    #theCategoryHeader is the category header and the categories correspond to the data in the row\n",
    "    theCategoryHeader = theSoup.find('div', attrs = {'style' : 'position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:39px; top:143px; width:63px; height:22px;'})\n",
    "    theCategoryHeaderText = theCategoryHeader.text\n",
    "    theCategories = theSoup.find('div', attrs = {'style' : 'position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:49px; top:461px; width:113px; height:47px;'})\n",
    "    #stripping all numbers and commas from the string\n",
    "    theCategoriesText = (''.join([theChar for theChar in theCategories.text if not (theChar.isdigit())])).replace(',','')\n",
    "    \n",
    "    print(theCategoryHeaderText[:7])\n",
    "    print(theCategoriesText)\n",
    "    print(theHeaderText[:3])\n",
    "    print(theDataText[41:])\n",
    "    \n",
    "    theCombinedCategoryText = theCategoryHeaderText[:7] + '\\n' + theCategoriesText\n",
    "    theCombinedData = theHeaderText[:3] + '\\n' + theDataText[41:]\n",
    "    for theCategoryLine, theDataLine in zip(theCombinedCategoryText.splitlines(), theCombinedData.splitlines()):\n",
    "        theRowData.append((theCategoryLine,theDataLine))\n",
    "        \n",
    "    with open(aFileName[:-5] + '.csv', 'a') as theOutputStream: \n",
    "        theWriter = csv.writer(theOutputStream)\n",
    "        for theCategory, theIndData in theRowData:\n",
    "            theWriter.writerow([theCategory, theIndData])\n",
    "     \n",
    "    with open(aFileName[:-5] + '.csv') as theInputStream:\n",
    "        theReader = csv.reader(theInputStream)\n",
    "        for theRow in theReader:\n",
    "            print(theRow)\n",
    "    \n",
    "Scraping_HTML_File('TheNevadaTable.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Function above takes an input html file, reads the data, selects the data required, and outputs it into a new csv file. I first used pdf2txt commandline command after calling the first function so the pdf dataset becomes a html file that I could scrape. I then initialize an empty list, theRowData, to which I will append individual row data in afterwards. \n",
    "\n",
    "[Example of an HTML file](TheCaliforniaTable.html) (disclaimer: IDK why it is not showing the html tags, but it should be similar to the Example of a Tag I look for)\n",
    "\n",
    "The next step I took was to create a beautiful soup of the html file so I could read the individual HTML tags. I then looked for the Category Header Tag, which is Measure in the above case, the categories, which are Serious Mental Illness, Any Mental Illness, Received Mental Health Services, Had Serious Thoughts of Suicide, and Major Depressive Episode, the Data Header, which is 18+, and lastly, the data itself, which in this instance are 93, 404, 256, 95, and 162. \n",
    "[Example of a Tag I look for](Div_Tag_Snip.png)\n",
    "\n",
    "All of the scraped data except for the categories is then cleaned by only selecting the data we want by string splicing, with each state having a different theDataText splicing and the other ones stay consistent.\n",
    "\n",
    "The Categories Text's numbers and commas are then filtered because we do not need them. Later, I combined the category header with the categories text by joining the with a new line in between and I undergo the same process for the data header and the data.\n",
    "\n",
    "Lastly, theRowData list is iterated through and each row is written into a new csv file that has the same name as the input filename, but a different file format. \n",
    "\n",
    "[Example of an Output CSV File](TheCaliforniaTable.csv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
